{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Thank you for agreeing to take part in this evaluation. During this evaluation you will be asked to carry out a series of tasks related to the dataset shown in the next section. Please carry out these tasks with the same care and rigor as if these tasks were part of your job duties.\n",
    "    \n",
    "Before beginning please ensure that you are using the prompter kernel by looking at the top right of this notebook. It should say \"prompter\", rather than \"Python 3\". If says you are using a python kernel, please click on where it says Python, and select the prompter from the drop down study. If you encounter any errors, or if it says \"No Kernel!\" please contact [PLACEHOLDER@uchicago.edu](PLACEHOLDER@uchicago.edu) so we can fix the issue.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Description\n",
    "\n",
    "This task is structured into four parts.\n",
    "\n",
    "1. **Dataset introduction**\n",
    "2. **Data cleaning**\n",
    "3. **Model Training**\n",
    "4. **Model Selection**\n",
    "\n",
    "In each of these, there will be some code pre-written. This code is meant to help you complete the task by providing structure, but you are not required to use the provided code if you do not want to. You may refer to any documentation source you like during this task (such as Pandas or scikit-learn API documentation, or StackOverflow).\n",
    "\n",
    "We ask that you use pandas and scikit-learn to perform the tasks. We have also installed numpy and matplotlib, should those be helpful. You will not be able to install any other non-standard libraries.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset Introduction\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be asking you to use the provided \"loan&#95;data.csv\" dataset during this experiment. This data was collected in a major metropolitan city in the United States. It contains information about applications for loans recieved by several different loan providers.\n",
    "\n",
    "Let's start trying to understand the dataset by writing some python code. Feel free to follow along by running the following sample commands in the data&#95;tasks.ipynb notebook which can be found on the left menu.\n",
    "\n",
    "Below is a few lines of python code that loads the provided \"loan&#95;data.csv\" dataset into a pandas dataframe. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "loans = pd.read_csv(\"loan_data.csv\")\n",
    "loans.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get a list of the columns in this dataframe with the following command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some of the columns in the dataframe. The column ``approved`` indicates whether or not the loan was approved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(loans[\"approved\"])/len(loans[\"approved\"]) # calculating the percent of approved loans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column ``principal`` is the amount of money the loan was for, that is how much money the applicant received if the loan was approved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 25th and 75th percentile\n",
    "print(loans[\"principal\"].quantile(0.25), loans[\"principal\"].quantile(0.75))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``interest`` is the annual percent interest on the loan. ``term`` is how long in months the loan was supposed to run. \n",
    "\n",
    "``income`` is the annual income of the loan applicants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``type`` column denotes the purpose of the loan. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans[\"type\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three possible values ``type`` can have: ``personal``, ``home`` and ``auto``. \n",
    "\n",
    "* ``auto`` These loans were for automobile purchases. With these loans, the lender may repossess the car if the person who took out the loan is unable to make payments.\n",
    "\n",
    "* ``home`` These loans were for the purchase of residential real estate. Under these loans, the lender may reposssess the home through foreclosure if the person who took out the loan is unable to make payments.\n",
    "\n",
    "* ``personal`` These loans are for personal expenses or investments other than a home or automobile. Under these loans, there is not generally any specific piece of property that a lender may repossess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we calculate the proportion of each type of loan\n",
    "\n",
    "# loans[\"type\"] == \"personal\" is an array of boolean values\n",
    "\n",
    "# since python treats True as a 1, and False as a 0, the sum\n",
    "# of this array is the number of entries in loans where the type equals \"personal\"\n",
    "\n",
    "personal_pct = sum(loans[\"type\"] == \"personal\")/len(loans[\"type\"])\n",
    "home_pct = sum(loans[\"type\"] == \"home\")/len(loans[\"type\"])\n",
    "auto_pct = sum(loans[\"type\"] == \"auto\")/len(loans[\"type\"])\n",
    "\n",
    "print(\"Personal loans: {0}\\nHome loans: {1}\\nAuto loans: {2}\\n\".format(personal_pct, home_pct, auto_pct))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your Turn\n",
    "\n",
    "At the end of the next sections, there will be space like this where we will ask you to complete certain tasks. \n",
    "\n",
    "There's nothing that you need to do right now. However if you want to examine any of the columns in more depth this is a good spot to do so. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is where you will write python code. Execute these cells using the\n",
    "#play button above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Cleaning\n",
    "\n",
    "Before we can use this data to build a model, we'll need to clean it up a bit. \n",
    "\n",
    "The ``date`` column is the date the loan was applied for. However because of the way Pandas works with csv files, this is interpreted as a string. In order do things like test whether a loan was issued before or after a certain date, we need to convert it to a different format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans[\"date\"] = pd.to_datetime(loans[\"date\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can easily find out how many applications there were from 2015 on using the same trick we used to get the proportion of different types of loans in the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "sum(loans[\"date\"] >= datetime(month=1, day=1, year=2015))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a. Categorical data\n",
    "Another thing we need to do is to deal with data that is categorical (not numeric and which does not have any sort of ordering). Since many scikit-learn models can only handle data that is numeric, it is necessary to represent these columns numerically.\n",
    "\n",
    "One example of this is the ``type`` column. To turn this into numeric data, we'll apply a dummy encoding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this encodes the type column of loans using a dummy coding\n",
    "coded_cols = pd.get_dummies(loans[\"type\"], \n",
    "                            prefix=\"type\", # this sets the column names as type_<var_name>\n",
    "                            drop_first=True) # we'll explain what this does below\n",
    "\n",
    "coded_cols.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row in this dataframe indicates whether the type was ``home`` or ``personal``.\n",
    "\n",
    "Maybe you've noticed that there are only two columns in this dataframe. That's because we set the ``drop_first`` argument in the ``get_dummies`` function to ``True``. Since there were three possible values for loan type, we know that any entry that has a 0 in both the ``type_home`` and ``type_personal`` columns is of type ``auto``.\n",
    "\n",
    "Why don't we also have a column indicating whether the loan is an auto loan or not? In some types of model, having a redundant column like that can cause problems. \n",
    "\n",
    "Now we need to attach these dummy columns back to our original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this attaches the coded columns \"to the side\" of loans\n",
    "encoded_loans = pd.concat([loans, coded_cols], axis=1) \n",
    "\n",
    "# but loans still contains the \"type\" column so we drop that \n",
    "# to keep things neat\n",
    "loans = encoded_loans.drop([\"type\"], axis=1)\n",
    "loans.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b. Null data\n",
    "\n",
    "Several columns contain null data. This means that the information was not recorded by the loan officer at the time of the application. This may be a problem as many machine learning models cannot handle undefined input or output values.\n",
    "\n",
    "There are several methods of handling null data. One option is to drop all rows where the entry is not defined. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dropped = loans.dropna()\n",
    "print(len(loans), len(all_dropped))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, this removed a significant number of entries. Doing this makes the data much easier to use, but may introduce systematic errors if the data is not missing in a purely random fashion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another option is to drop just the columns where there is a null entry. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_dropped = loans.dropna(axis=\"columns\")\n",
    "\n",
    "# which columns are removed this way?\n",
    "missing = [l for l in loans.columns if l not in cols_dropped.columns]\n",
    "\n",
    "print(missing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This preserved the number of entries, but means that we're missing possibly important columns like ``income``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also try to attribute a value to data in a column that's missing. Here for example, we fill the missing entries in the ``income`` column with 0.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filled_interest = loans[\"income\"].fillna(0.0)\n",
    "filled_interest.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your Turn\n",
    "\n",
    "There are two things you need to do here \n",
    "\n",
    "1. Handle the missing data in the \"loan_data.csv\". We suggest that you do this first as it may impact how you handle encoding the types of the data. \n",
    "\n",
    "2. Look through the columns and encode data into the correct type. A helpful command to see the type of each of the columns in the dataframe is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write your code here to perform part 1 of the task. Below is a small\n",
    "#code snippet to help you get started. You may delete the snippet\n",
    "#if you wish.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def clean_data(loans_dataframe):\n",
    "    '''function for cleaning '''\n",
    "    for c in loans_dataframe.columns:\n",
    "        #do something to the data\n",
    "        print(c)\n",
    "    return loans_dataframe\n",
    "        \n",
    "clean_data(pd.read_csv(\"loan_data.csv\"))\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When completing both 1 & 2, you are free to use any method mentioned here. If you want to use a technique or method not mentioned here, you are also free to do so. You are also free to revisit this or any other section at any time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write your code here to perform part 2 of the task\n",
    "#Below is a small code snippet to help you get started,\n",
    "#you may delete the snippet if you wish\n",
    "\n",
    "def encode_data(cleaned_loans_dataframe):\n",
    "        print(cleaned_loans_dataframe.dtypes)\n",
    "        #do something\n",
    "        return cleaned_loans_dataframe\n",
    "        \n",
    "cleaned_loans = clean_data(pd.read_csv(\"loan_data.csv\"))        \n",
    "encode_data(cleaned_loans)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Do not uncomment the following line of code. Make Sure to run the following line of code as is.** This tells the plugin that you have reached the stage where you are starting to build models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# %prompter_plugin model_training%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Executing this block does not prevent you from going back and revisiting previous data analysis decisions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In section 4, we will ask you to train a classifier which predicts whether a loan will be approved or not. The purpose of this classifier is to be used by loan officers or local lenders to recommend to applicants specific loans they might be eligible for. The lenders hope that this tool will help match financial products to borrowers more efficiently. \n",
    "\n",
    "Please note that this classifier *will not* be used to make decisions about whether to grant a loan or not. \n",
    "\n",
    "There are a couple different things you could do here. A small example using the ``LogisticRegression`` is included below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = loans[[\"type_home\", \"type_personal\", \"principal\", \"interest\"]]\n",
    "y = loans[\"approved\"]\n",
    "\n",
    "# this creates training and testing sets \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train,y_train) # this \"fits\" or \"trains\" the model using X and y as training data\n",
    "\n",
    "# this is the predictions for what y should be. \n",
    "# they may be helpful if you want to understand more about what a model is doing\n",
    "preds = lr.predict(X_test)\n",
    "\n",
    "lr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are other types of classifiers in scikit-learn, like the KNeighborsClassifier, or the DecisionTreeClassifier. If you are not familiar with these, don't worry. They all may be trained using the same calls to ``fit``, ``score`` and ``predict``.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your Turn\n",
    "\n",
    "How well does the regression we built work? Is an accuracy of 0.43 good or bad in this case?\n",
    "\n",
    "Use the data you cleaned up in section 2 to build your own Logistic Regression. In the regression we built above, we just used a few columns. Try different combinations of columns to see if that changes the results you get. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also try changing the type of model that you're using. In the example above, we just used the LogisticRegression classifier. Do you get different results if you use a different type of classifier?\n",
    "\n",
    "In the cell below, we've give you some suggestions about some different classifiers to try. If you've used these before, want to try a different type of classifier, or want to use non-default parameters, feel free to do so. \n",
    "\n",
    "If you haven't used these before, remember that they have the same ``fit``, ``predict`` and ``score`` functions as the logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In section 3 you should have explored the performance of several different classifiers. Now is the time where you need to choose which one you believe to be the best. To submit your model for consideration, please execute the code below. Do not uncomment it, but do replace ``MODEL_NAME`` with the variable name that refers to the model you would like to submit.\n",
    "\n",
    "Remember that the model you submit will be used to recommend financial products to prospective loan applicants. \n",
    "\n",
    "You may submit multiple times, however only your last submitted model will be considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %prompter_plugin submit MODEL_NAME %"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End\n",
    "\n",
    "Thank you for participating in the study. You please return to the Qualtrics and complete the survey."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
