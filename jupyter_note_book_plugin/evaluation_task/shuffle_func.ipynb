{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c81d35d5-f9d1-46ea-a219-c4c3a8b862a3",
   "metadata": {
    "section": "intro",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def permute_data(data, one_hot_cols):\n",
    "    \"\"\"\n",
    "    for each row, choose a column unif. at random and change a value\n",
    "    \"\"\"\n",
    "    all_one_hot = [c for col_set in one_hot_cols for c in col_set]\n",
    "    normal_cols = [c for c in data.columns if c not in all_one_hot]\n",
    "    \n",
    "    def permute_row(row):\n",
    "        # choose which column\n",
    "        chosen = np.random.choice(len(normal_cols)+len(one_hot_cols))\n",
    "        if chosen < len(normal_cols):\n",
    "            col = normal_cols[chosen]\n",
    "            row[col] = np.random.choice(data[col])\n",
    "        else:\n",
    "                        \n",
    "            col = one_hot_cols[chosen-len(normal_cols)]\n",
    "            counts = data[col].sum(axis=0)\n",
    "            \n",
    "            # may be dummy coded with reference class = 0\n",
    "            if counts.sum() == len(data):\n",
    "                probs = counts/len(data)\n",
    "                col_choice = np.random.choice(col, p=probs)\n",
    "            \n",
    "                row[col] = 0\n",
    "                row[col_choice] = 1.0\n",
    "            else:\n",
    "                counts = list(counts) + [len(data) - counts.sum()]\n",
    "                probs = np.array(counts)/len(data)\n",
    "                \n",
    "                col_choices = col + [None]\n",
    "                col_choice = np.random.choice(col_choices, p=probs)\n",
    "                \n",
    "                row[col] = 0\n",
    "                \n",
    "                if col_choice is not None:\n",
    "                    row[col_choice] = 1.0\n",
    "                \n",
    "        return row\n",
    "    return data.apply(permute_row, axis=1)\n",
    "\n",
    "def test_data_perturb(model, input_features, one_hot_cols, n_tests=10):\n",
    "    \"\"\"\n",
    "    resample the data n_tests times and return the true->false and false->true\n",
    "    \n",
    "    assume labels are true/false\n",
    "    \n",
    "    one_hot_cols is a (possibly empty) list of lists of columns that are one-hot encoded\n",
    "    \"\"\"\n",
    "    \n",
    "    preds = model.predict(input_features)\n",
    "    false_to_true = 0\n",
    "    true_to_false = 0\n",
    "    \n",
    "    for _ in range(n_tests):\n",
    "        \n",
    "        shuff_data = permute_data(input_features, one_hot_cols)\n",
    "        shuff_preds = model.predict(shuff_data)\n",
    "        \n",
    "        false_to_true += (~preds & shuff_preds).sum()\n",
    "        true_to_false += (preds & ~ shuff_preds).sum()\n",
    "        \n",
    "    return false_to_true/(n_tests*len(preds)), true_to_false/(n_tests*len(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fbffba0c-e06f-498f-b00b-2b132802e2d1",
   "metadata": {
    "section": "intro"
   },
   "outputs": [],
   "source": [
    "def make_metrics(true,preds):\n",
    "    \n",
    "    fp = (~true & preds).sum()\n",
    "    tn = (~true & ~preds).sum()\n",
    "    tp = (true & preds).sum()\n",
    "    fn = (true & ~preds).sum()\n",
    "    \n",
    "    precision = tp/(tp+fp)\n",
    "    recall = tp/(tp+fn)\n",
    "    fpr = fp/(fp+tn)\n",
    "    fnr = fn/(tp+fn)\n",
    "    f1 = 2*(precision*recall)/(precision + recall)\n",
    "    \n",
    "    return precision, recall, fpr, fnr, f1\n",
    "\n",
    "def make_model_report(data, true, preds, model, input_features,\n",
    "                     one_hot_cols):\n",
    "    \"\"\"\n",
    "    do report for values and save as a dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    # precision\n",
    "    # recall\n",
    "    # FPR\n",
    "    # FNR\n",
    "    # F1\n",
    "    \n",
    "    precision, recall, fpr, fnr, f1 = make_metrics(true, preds)\n",
    "    perf_df = pd.DataFrame({\"model_precision\" : [precision],\n",
    "                  \"model_recall\" : [recall],\n",
    "                  \"model_FPR\" : [fpr],\n",
    "                  \"model_FNR\" : [fnr],\n",
    "                  \"model_F1\" : [f1]})\n",
    "    \n",
    "    for col in [\"race\", \"gender\"]:\n",
    "        \n",
    "        scores = {\"precision\" : [],\n",
    "                  \"recall\" : [],\n",
    "                  \"FPR\" : [],\n",
    "                  \"FNR\" : [],\n",
    "                  \"F1\" : []}\n",
    "        \n",
    "        for val in data[col].unique():\n",
    "            \n",
    "            subset = data[col] == val\n",
    "            precision, recall, fpr, fnr, f1 = make_metrics(true[subset], preds[subset])\n",
    "            \n",
    "            scores[\"precision\"].append((precision, val))\n",
    "            scores[\"recall\"].append((recall, val))\n",
    "            scores[\"FPR\"].append((fpr, val))\n",
    "            scores[\"FNR\"].append((fnr, val))\n",
    "            scores[\"F1\"].append((f1, val))\n",
    "                \n",
    "        for k in scores:\n",
    "            \n",
    "            max_tuple = max(scores[k], key=lambda x: x[0])\n",
    "            min_tuple = min(scores[k], key=lambda x: x[0])\n",
    "            \n",
    "            perf_df[f\"{k}_{col}_min\"] = [min_tuple[0]]\n",
    "            perf_df[f\"{col}_with_min_{k}\"] = [min_tuple[1]]\n",
    "            \n",
    "            perf_df[f\"{k}_{col}_max\"] = [max_tuple[0]]\n",
    "            perf_df[f\"{col}_with_max_{k}\"] = [max_tuple[1]]\n",
    "    \n",
    "    \n",
    "    false_true, true_false = test_data_perturb(model, input_features, one_hot_cols)\n",
    "    perf_df[\"model_pertubation_true_false_rate\"] = [true_false]\n",
    "    perf_df[\"model_pertubation_false_true_rate\"] = [false_true]\n",
    "    perf_df.to_csv(\"./perf_df.csv\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "616d4038-0a36-494f-bc3a-7f4390a532ea",
   "metadata": {
    "section": "intro"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "column_types = {\n",
    "    \"race\" : \"category\",\n",
    "    \"gender\" : \"category\",\n",
    "    \"zip\" : \"category\",\n",
    "    \"income\" : float,\n",
    "    \"type\" : \"category\",\n",
    "    \"interest\" : float,\n",
    "    \"term\" : float,\n",
    "    \"principal\" : float,\n",
    "    \"approved\" : bool,\n",
    "    \"adj_bls_2\" : float,\n",
    "    \"id\" : str,\n",
    "}\n",
    "data = pd.read_csv(\"./clean_data.csv\", dtype=column_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b198285-587d-45ea-bbf0-854b10fd0692",
   "metadata": {
    "section": "intro"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "X = data[[\"income\", \"interest\", \"term\", \"adj_bls_2\"]]\n",
    "X_cat = pd.get_dummies(data[\"type\"], drop_first=True) \n",
    "X_gend = pd.get_dummies(data[\"gender\"])\n",
    "X = pd.concat([X, X_cat, X_gend], axis=1)\n",
    "y = data[\"approved\"]\n",
    "\n",
    "lr.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3a7d5cb4-9b66-4a40-9e43-f344b7ec4d40",
   "metadata": {
    "section": "intro"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.061375, 0.04783333333333333)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_perturb(lr, X, [[\"home\", \"personal\"], [\"female\", \"male\", \"non-binary\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0064b0dd-3bbf-4d89-80a9-dbfa67738177",
   "metadata": {
    "section": "intro"
   },
   "outputs": [],
   "source": [
    "preds = lr.predict(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "025dc3db-ff1b-42c2-add9-08322b7b6a2f",
   "metadata": {
    "section": "intro"
   },
   "outputs": [],
   "source": [
    "make_model_report(data, y, preds, lr, X,  [[\"home\", \"personal\"], [\"female\", \"male\", \"non-binary\"]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "retrograde",
   "language": "",
   "name": "prompt_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
