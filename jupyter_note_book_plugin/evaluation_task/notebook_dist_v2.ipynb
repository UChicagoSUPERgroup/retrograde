{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "new_section": "intro_start",
    "section": "intro"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "Thank you for agreeing to take part in this evaluation. During this evaluation you will be asked to carry out a series of tasks related to the dataset shown in the next section. Please carry out these tasks with the same care and rigor as if these tasks were part of your job duties.\n",
    "    \n",
    "Before beginning please ensure that you are using the prompter kernel by looking at the top right of this notebook. It should say \"prompter\", rather than \"Python 3\". If says you are using a python kernel, please click on where it says Python, and select the prompter from the drop down study. If you encounter any errors, or if it says \"No Kernel!\" please contact [retrograde-plugin@uchicago.edu](retrograde-plugin@uchicago.edu) so we can fix the issue.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "section": "intro"
   },
   "source": [
    "## Task Description\n",
    "\n",
    "This task is structured into four parts.\n",
    "\n",
    "1. **Dataset introduction**\n",
    "2. **Data cleaning**\n",
    "3. **Model Training**\n",
    "4. **Model Selection**\n",
    "\n",
    "In each of these four sections, there is a tutorial portion and a task portion.\n",
    "The tutorial portions are meant to provide background and structure to the task, and may be helpful to you when completing the tasks.\n",
    "You will know that you have reached a task portion because we will mark them <span style=\"color:red\"> **in red** </span>\n",
    "\n",
    "In each of these, there will be some code pre-written. This code is meant to help you complete the task by providing structure, but you are not required to use the provided code if you do not want to. You may refer to any documentation source you like during this task (such as StackOverflow, or Pandas API documentation or tutorials).\n",
    "\n",
    "We ask that you use pandas and scikit-learn to perform the tasks. We have also installed numpy and matplotlib, should those be helpful. You will not be able to install any other non-standard libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "new_section": "tutorial_start",
    "section": "intro"
   },
   "source": [
    "## 1. Dataset Introduction\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "section": "intro"
   },
   "source": [
    "We will be asking you to use the provided \"loan&#95;data.csv\" dataset during this experiment. This data was collected in a major metropolitan city in the United States. It contains information about applications for loans recieved by several different loan providers.\n",
    "\n",
    "Let's start trying to understand the dataset by writing some python code. Feel free to follow along by running the following sample commands in this notebook.  \n",
    "\n",
    "Below is a few lines of python code that loads the provided \"loan&#95;data.csv\" dataset into a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "section": "intro"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# What each of these columns represents is explained below. This dictionary tells pandas what \n",
    "# data type each of the columns should be treated as.\n",
    "\n",
    "column_types = {\n",
    "    \"race\" : \"category\",\n",
    "    \"gender\" : \"category\",\n",
    "    \"zip\" : \"category\",\n",
    "    \"income\" : float,\n",
    "    \"type\" : \"category\",\n",
    "    \"term\" : int,\n",
    "    \"interest\" : float,\n",
    "    \"principal\" : int,\n",
    "    \"approved\" : bool,\n",
    "    \"adj_bls_2\" : float,\n",
    "    \"id\" : str,\n",
    "}\n",
    "\n",
    "loans = pd.read_csv(\"loan_data.csv\", parse_dates=[\"date\"], dtype=column_types)\n",
    "loans.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "section": "intro"
   },
   "source": [
    "We can get a list of the columns in this dataframe with the following command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "section": "intro"
   },
   "outputs": [],
   "source": [
    "loans.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "section": "intro"
   },
   "source": [
    "Let's look at some of the columns in the dataframe. The column ``approved`` indicates whether or not the loan was approved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "section": "intro"
   },
   "outputs": [],
   "source": [
    "# since python treats True as a 1, and False as a 0, the sum\n",
    "# of this array is the number of entries in loans where approved == True\n",
    "\n",
    "sum(loans[\"approved\"])/len(loans[\"approved\"]) # calculating the fraction of approved loans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "section": "intro"
   },
   "source": [
    "The column ``principal`` is the amount of money the loan was for, that is how much money the applicant received if the loan was approved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "section": "intro"
   },
   "outputs": [],
   "source": [
    "# 25th and 75th percentile\n",
    "print(loans[\"principal\"].quantile(0.25), loans[\"principal\"].quantile(0.75))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "section": "intro"
   },
   "source": [
    "That's a fairly wide variation in loan amounts. One possible reason for this is that there are different purposes for which the loans were applied for. The ``type`` column denotes the purpose of the loan. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "section": "intro"
   },
   "outputs": [],
   "source": [
    "loans[\"type\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "section": "intro"
   },
   "source": [
    "There are three possible values ``type`` can have: ``personal``, ``home`` and ``auto``. \n",
    "\n",
    "* ``auto`` These loans were for automobile purchases. With these loans, the lender may repossess the car if the person who took out the loan is unable to make payments.\n",
    "\n",
    "* ``home`` These loans were for the purchase of residential real estate. Under these loans, the lender may reposssess the home through foreclosure if the person who took out the loan is unable to make payments.\n",
    "\n",
    "* ``personal`` These loans are for personal expenses or investments other than a home or automobile. Under these loans, there is not generally any specific piece of property that a lender may repossess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "section": "intro"
   },
   "outputs": [],
   "source": [
    "# here we use the same trick we used to calculate the approval rate \n",
    "# to calculate the different types of loans. \n",
    "\n",
    "personal_pct = sum(loans[\"type\"] == \"personal\")/len(loans[\"type\"])\n",
    "\n",
    "# Since the 'type' column is a string, the statement \n",
    "# loans[\"type\"] == \"personal\" produces a #Series of True/False values \n",
    "# indicating for each entry in the 'type' column whether it equals \"personal\"\n",
    "# or not\n",
    "\n",
    "home_pct = sum(loans[\"type\"] == \"home\")/len(loans[\"type\"])\n",
    "auto_pct = sum(loans[\"type\"] == \"auto\")/len(loans[\"type\"])\n",
    "\n",
    "print(\"Personal loans: {0}\\nHome loans: {1}\\nAuto loans: {2}\\n\".format(personal_pct, home_pct, auto_pct))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "section": "intro"
   },
   "source": [
    "``interest`` is the annual percent interest on the loan. ``term`` is how long in months the loan was supposed to run. \n",
    "\n",
    "``income`` is the annual income of the loan applicants.\n",
    "\n",
    "The file ``loan_data_dictionary.txt`` in this directory is a text file with a description of all of the columns in ``loan_data.csv``. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "section": "intro"
   },
   "source": [
    "### <span style=\"color:red\">Your Turn</span>\n",
    "\n",
    "Section 1 is just an introduction to the dataset you will be using. There's no specific task for you to do in this section. However, you should have a sense of what this data is and where it comes from before proceeding.\n",
    "\n",
    "In the next section, you will be asked to make certain decisions about how to clean this data and get it into a usable form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "new_section": "null_clean_start",
    "section": "intro"
   },
   "source": [
    "## 2. Data Cleaning\n",
    "\n",
    "Before we can use this data to build a model, we'll need to clean it up a bit. \n",
    "\n",
    "Several columns contain null data. This means that the information was not recorded by the loan officer at the time of the application. This may be a problem as many machine learning models cannot handle undefined input or output values.\n",
    "\n",
    "You may also find it useful to revisit your work in this section as you move on to the model building tasks in section 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "section": "intro"
   },
   "source": [
    "\n",
    "There are several methods of handling null data. One option is to drop all rows where the entry is not defined. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "section": "intro"
   },
   "outputs": [],
   "source": [
    "all_dropped = loans.dropna()\n",
    "print(len(loans), len(all_dropped))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "section": "intro"
   },
   "source": [
    "As you can see, this removed a significant number of entries. Doing this makes the data much easier to use, but may introduce systematic errors if the data is not missing in a purely random fashion. For example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "section": "intro"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "true_data = np.array([10, 40, 36, 12, 67])\n",
    "missing_data_lower = np.array([np.nan, 40, 36, np.nan, 67]) # lower values are missing\n",
    "missing_data_upper = np.array([10, np.nan, 36, 12, np.nan]) # upper values are missing\n",
    "print(np.nanmean(missing_data_upper), np.nanmean(missing_data_lower)) # nanmean computes the mean, but ignores nan or missing data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "section": "intro"
   },
   "source": [
    "Another option is to drop just the columns where there is a null entry. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "section": "intro"
   },
   "outputs": [],
   "source": [
    "cols_dropped = loans.dropna(axis=\"columns\")\n",
    "\n",
    "# which columns are removed this way?\n",
    "missing = [l for l in loans.columns if l not in cols_dropped.columns]\n",
    "\n",
    "print(missing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "section": "intro"
   },
   "source": [
    "This preserved the number of entries, but means that we're missing possibly important columns like ``income``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "section": "intro"
   },
   "source": [
    "You can also try to attribute a value to data in a column that's missing. Here for example, we fill the missing entries in the ``income`` column with 0.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "section": "intro"
   },
   "outputs": [],
   "source": [
    "filled_interest = loans[\"income\"].fillna(0.0)\n",
    "filled_interest.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "section": "intro"
   },
   "source": [
    "Note that some columns in the dataframe are encoded as *categorical* data.  In order to fill missing values in columns of these types, you must either add a value to the categories, or fill with a value already in the column.\n",
    "\n",
    "An example of this is gender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "section": "intro"
   },
   "outputs": [],
   "source": [
    "loans[\"gender\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "section": "intro"
   },
   "outputs": [],
   "source": [
    "loans[\"gender\"].cat.add_categories(\"missing\").fillna(\"missing\").unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "section": "intro"
   },
   "source": [
    "### <span style=\"color:red\">Your Turn</span>\n",
    "\n",
    "Handle the missing data in the \"loan_data.csv\". We suggest that you do this first as it may impact how you handle the model training in section 3.\n",
    "\n",
    "You are free to use any method mentioned here. If you want to use a technique or method not mentioned here, you are also free to do so. You are also free to revisit this or any other section at any time.\n",
    "\n",
    "**Remember**, when completing the tasks try to treat them as if they are part of your job, and it is your responsibility to create an effective model for predicting loan acceptance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "section": "intro"
   },
   "outputs": [],
   "source": [
    "#write your code here to perform the task. Below is a small\n",
    "#code snippet to help you get started. You may delete the snippet\n",
    "#if you wish.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def handle_nulls(loans_dataframe):\n",
    "    '''function for cleaning '''\n",
    "    # this just prints the columns with nulls\n",
    "    cols_with_nulls = [c for c in loans_dataframe.columns if loans_dataframe[c].isna().any()]\n",
    "    print(cols_with_nulls)\n",
    "    \n",
    "    # this just prints the number of rows with null data in any entry\n",
    "    rows_with_nulls = loans_dataframe.isna().any(axis=1)\n",
    "    \n",
    "    # loans_dataframe.isna() produces a dataframe of the same size and \n",
    "    # shape as loans_dataframe, but with True/False in each entry indicating\n",
    "    # whether each entry is null or not. \n",
    "    \n",
    "    # .any checks if any of the entries are true, with the axis argument set to \n",
    "    # 1, this means that it checks for each row in loans_dataframe.any() if there is \n",
    "    # an entry with True in it\n",
    "    \n",
    "    print(sum(rows_with_nulls))\n",
    "    \n",
    "    # Now you should do something to the data\n",
    "    \n",
    "    return loans_dataframe\n",
    "        \n",
    "cleaned_data = handle_nulls(pd.read_csv(\"loan_data.csv\", parse_dates=[\"date\"], dtype=column_types))\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "new_section": "model_start",
    "section": "intro"
   },
   "source": [
    "## 3. Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "section": "intro"
   },
   "source": [
    "In this section, we will ask you to train a classifier which predicts whether a loan will be approved or not. The purpose of this classifier is to be used by loan officers or local lenders to recommend to applicants specific loans they might be eligible for. The lenders hope that this tool will help match financial products to borrowers more efficiently. \n",
    "\n",
    "Please note that this classifier be used to make *recommendations* and **not** decisions about whether to grant a loan or not. \n",
    "\n",
    "In this part, there are two <span style=\"color:red\">**Your Turn**</span> sections. In the first, you will be asked to select features to use. In the second, you will be asked to select different model architectures. While we suggest completing the first one, and then moving on to the second one, and then revisiting the first and any other previous steps as necessary. \n",
    "\n",
    "There are a couple different things you could do here. A small example using the ``DummyClassifier`` is included below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "section": "intro"
   },
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_num = loans[[\"principal\", \"interest\"]] # these columns are numeric\n",
    "\n",
    "# since loan type is a categorical variable, we need to encode it numerically\n",
    "# this creates three columns of 0/1 denoting the type of loan each row is\n",
    "X_cat = pd.get_dummies(loans[\"type\"], prefix=\"type\")\n",
    "\n",
    "X = pd.concat([X_num, X_cat], axis=1) # this combines the categorical and numeric columns back into 1 dataframe\n",
    "y = loans[\"approved\"]\n",
    "\n",
    "# this creates training and testing sets \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)\n",
    "\n",
    "# dummy classifier guesses randomly at the labels\n",
    "clf = DummyClassifier(strategy=\"uniform\")\n",
    "clf.fit(X_train,y_train) # this \"fits\" or \"trains\" the model using X and y as training data\n",
    "\n",
    "# this is the prediction for what y should be. \n",
    "# they may be helpful if you want to understand more about what a model is doing\n",
    "preds = clf.predict(X_test)\n",
    "\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "section": "intro"
   },
   "source": [
    "We need to figure out if this score is good or bad. If you followed along from the start, you might recall that the approval rate in our data was about 0.43 as well. \n",
    "\n",
    "What if we tried using a model that always guesses the most frequent decision (to reject?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "section": "intro"
   },
   "outputs": [],
   "source": [
    "clf_unif = DummyClassifier(strategy=\"most_frequent\")\n",
    "clf_unif.fit(X_train, y_train)\n",
    "clf_unif.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "section": "intro"
   },
   "source": [
    "So it turns out that ``clf`` doesn't work very well. In fact, the model that guessed ``False`` all the time had a higher accuracy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "section": "intro"
   },
   "source": [
    "We can likely improve upon this baseline. In the next **Your Turn** we will ask you to explore some of the ways in which might improve the classification performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "section": "intro"
   },
   "source": [
    "### <span style=\"color:red\">Your Turn, Feature Selection</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "section": "intro",
    "tags": []
   },
   "source": [
    "Use the data you cleaned up in section 2 to build your own Logistic Regression. In the regression we built in the example above, we just used a few columns. Try different combinations of columns to see if that changes the results you get. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "section": "intro"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def choose_columns(cleaned_data):\n",
    "    # write code here\n",
    "    # Note: you may need to dummy code any non-numeric columns\n",
    "    return cleaned_data[numeric_columns]\n",
    "\n",
    "X = choose_columns(cleaned_data)\n",
    "y = cleaned_data[\"approved\"] # change this if you've renamed the `approved` column\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)\n",
    "\n",
    "# LogisiticRegression is another sklearn classifier\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train,y_train)\n",
    "lr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "section": "intro"
   },
   "source": [
    "### Model Selection\n",
    "\n",
    "You can also try changing the type of model that you're using. In the example above, we just used the LogisticRegression classifier. Do you get different results if you use a different type of classifier?\n",
    "\n",
    "There are other types of classifiers in scikit-learn, like the KNeighborsClassifier, or the DecisionTreeClassifier. If you are not familiar with these, don't worry. They all may be trained using the same calls to ``fit``, ``score`` and ``predict``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "section": "intro"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier()\n",
    "knn = KNeighborsClassifier()\n",
    "lr = LogisticRegression()\n",
    "\n",
    "X = choose_columns(cleaned_data)\n",
    "y = cleaned_data[\"approved\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)\n",
    "\n",
    "dt.fit(X_train, y_train)\n",
    "knn.fit(X_train, y_train)\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "print(lr.score(X_test, y_test), dt.score(X_test, y_test), knn.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "section": "intro"
   },
   "source": [
    "### <span style=\"color:red\">Your Turn, Model Selection</span>\n",
    "\n",
    "Use the data and features you selected in the previous sections to train different types of models. You may use any scikit-learn model you see fit. If you think it might be useful, you can also try different parameters for different models. If you don't know what a model parameter is, or what the model parameter means, you don't need to worry about that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "section": "intro"
   },
   "outputs": [],
   "source": [
    "# these are some examples of classifiers from the scikit-learn library\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "X = choose_columns(cleaned_data)\n",
    "y = cleaned_data[\"approved\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)\n",
    "\n",
    "# put models here \n",
    "rf = RandomForestClassifier().fit(X_train, y_train)\n",
    "\n",
    "print(rf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "new_section": "end_start",
    "section": "intro"
   },
   "source": [
    "# End\n",
    "\n",
    "Thank you for participating in the study. In section 3 you should have explored the performance of several different classifiers. Now is the time where you need to choose which one you believe to be the best. To submit your model for consideration, please assign your model to the variable SUBMITTED_MODEL in the cell below and then execute the cell.\n",
    "\n",
    "Remember that the model you submit will be used to recommend financial products to prospective loan applicants. \n",
    "\n",
    "You may submit multiple times, however only your last submitted model will be considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "section": "intro"
   },
   "outputs": [],
   "source": [
    "SUBMITTED_MODEL = # write the variable here\n",
    "\n",
    "# if the model you decide on is named clf, then you would write \n",
    "# SUBMITTED_MODEL = clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "section": "intro"
   },
   "source": [
    "You may now return to the Qualtrics and complete the survey."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
